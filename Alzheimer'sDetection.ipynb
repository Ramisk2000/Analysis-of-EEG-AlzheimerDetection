{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554fab0d-103a-4907-9610-f90856c607bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0        1        2        3        4        5       6        7  \\\n",
      "0 -2.90680 -6.69830 -5.75040 -2.90680 -1.95890 -0.06319 -1.0111 -4.80250   \n",
      "1 -0.88889 -2.79370 -5.65080 -4.69840 -1.84130  1.01590  1.0159 -2.79370   \n",
      "2 -6.82930 -8.78050 -6.82930 -2.92680  0.00000  0.00000 -2.9268 -5.85370   \n",
      "3  1.92310  0.96154  0.96154  0.96154  0.96154  2.88460  2.8846 -0.96154   \n",
      "4  0.00000 -2.94120 -1.96080 -0.98039  1.96080  2.94120  5.8824  1.96080   \n",
      "\n",
      "         8        9  ...     1018     1019     1020    1021     1022    1023  \\\n",
      "0 -3.85470 -1.01110  ... -0.06319 -0.06319 -0.06319 -1.0111 -1.95890 -2.9068   \n",
      "1 -2.79370 -0.88889  ... -1.84130 -3.74600 -3.74600 -1.8413  0.06349  1.9683   \n",
      "2 -2.92680  0.97561  ...  0.00000  0.97561  0.97561  0.0000 -1.95120 -1.9512   \n",
      "3  0.00000  0.00000  ...  1.92310 -1.92310 -2.88460 -2.8846 -3.84620 -4.8077   \n",
      "4  0.98039  1.96080  ...  7.84310  3.92160  3.92160  2.9412  3.92160  3.9216   \n",
      "\n",
      "   condition        state    patient  \\\n",
      "0         AD  Eyes_closed  Paciente1   \n",
      "1         AD  Eyes_closed  Paciente1   \n",
      "2         AD  Eyes_closed  Paciente1   \n",
      "3         AD  Eyes_closed  Paciente1   \n",
      "4         AD  Eyes_closed  Paciente1   \n",
      "\n",
      "                                   electrode  \n",
      "0          C3 - Central Left (Motor Control)  \n",
      "1         C4 - Central Right (Motor Control)  \n",
      "2  Cz - Central Midline (Motor Coordination)  \n",
      "3        F1 - Frontal Left (Decision Making)  \n",
      "4       F2 - Frontal Right (Decision Making)  \n",
      "\n",
      "[5 rows x 1028 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ELECTRODE_INFO = {\n",
    "    'C3': 'C3 - Central Left (Motor Control)',\n",
    "    'C4': 'C4 - Central Right (Motor Control)',\n",
    "    'Cz': 'Cz - Central Midline (Motor Coordination)',\n",
    "    'F1': 'F1 - Frontal Left (Decision Making)',\n",
    "    'F2': 'F2 - Frontal Right (Decision Making)',\n",
    "    'F3': 'F3 - Frontal Left (Cognitive Function)',\n",
    "    'F4': 'F4 - Frontal Right (Cognitive Function)',\n",
    "    'F7': 'F7 - Frontal Left (Emotional Control)',\n",
    "    'F8': 'F8 - Frontal Right (Emotional Control)',\n",
    "    'Fp1': 'Fp1 - Frontal Pole Left (Attention)',\n",
    "    'Fp2': 'Fp2 - Frontal Pole Right (Attention)',\n",
    "    'Fz': 'Fz - Frontal Midline (Executive Function)',\n",
    "    'O1': 'O1 - Occipital Left (Visual Processing)',\n",
    "    'O2': 'O2 - Occipital Right (Visual Processing)',\n",
    "    'P3': 'P3 - Parietal Left (Sensory Integration)',\n",
    "    'P4': 'P4 - Parietal Right (Sensory Integration)',\n",
    "    'Pz': 'Pz - Parietal Midline (Spatial Awareness)',\n",
    "    'T3': 'T3 - Temporal Left (Language Comprehension)',\n",
    "    'T4': 'T4 - Temporal Right (Language Comprehension)',\n",
    "    'T5': 'T5 - Temporal Posterior Left (Memory)',\n",
    "    'T6': 'T6 - Temporal Posterior Right (Memory)'\n",
    "}\n",
    "\n",
    "data_folder = 'C:/Users/ASUS/Desktop/EEG_data'\n",
    "all_data = []  # Initialize all_data as an empty list\n",
    "labels = []    # Initialize labels as an empty list\n",
    "\n",
    "# Specify the extension you want to load (e.g., '.txt')\n",
    "file_extension = '.txt'\n",
    "\n",
    "# Load signals and label them with condition, state, and electrode information\n",
    "for condition in ['AD', 'Healthy']:\n",
    "    for state in ['Eyes_closed', 'Eyes_open']:\n",
    "        state_path = os.path.join(data_folder, condition, state)\n",
    "\n",
    "        # Dynamically handle any number of Paciente folders\n",
    "        for patient_folder in os.listdir(state_path):\n",
    "            patient_path = os.path.join(state_path, patient_folder)\n",
    "            \n",
    "            # Iterate through each electrode file within the patient's folder\n",
    "            for electrode in ELECTRODE_INFO.keys():\n",
    "                # Look for files named like 'C3.txt', 'C4.txt', etc.\n",
    "                electrode_file = f\"{electrode}{file_extension}\"\n",
    "                file_path = os.path.join(patient_path, electrode_file)\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        signal = np.loadtxt(file_path)\n",
    "                        all_data.append(signal)\n",
    "                        # Append the condition, state, patient, and electrode information as labels\n",
    "                        labels.append([condition, state, patient_folder, ELECTRODE_INFO[electrode]])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Check if any data was loaded\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Add condition, state, patient, and electrode as columns\n",
    "    df['condition'] = [label[0] for label in labels]\n",
    "    df['state'] = [label[1] for label in labels]\n",
    "    df['patient'] = [label[2] for label in labels]\n",
    "    df['electrode'] = [label[3] for label in labels]\n",
    "\n",
    "    # Display the first few rows of the data\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No data loaded. Check folder paths and file contents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8efa22-0601-4f20-8ff8-22037afe2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      0.96      0.96       672\n",
      "     Healthy       0.73      0.72      0.73        97\n",
      "\n",
      "    accuracy                           0.93       769\n",
      "   macro avg       0.84      0.84      0.84       769\n",
      "weighted avg       0.93      0.93      0.93       769\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[646  26]\n",
      " [ 27  70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Flatten the EEG signal data (each 1024-value list becomes a single feature vector)\n",
    "X = np.array([np.array(signal).flatten() for signal in all_data])  # Features (EEG signals)\n",
    "y = np.array([label[0] for label in labels])  # Labels (AD or Healthy)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c92326-9f72-4bd4-8207-73fcdc293f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.97\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      1.00      0.98       672\n",
      "     Healthy       1.00      0.73      0.85        97\n",
      "\n",
      "    accuracy                           0.97       769\n",
      "   macro avg       0.98      0.87      0.91       769\n",
      "weighted avg       0.97      0.97      0.96       769\n",
      "\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "[[672   0]\n",
      " [ 26  71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e81f9be-4da5-49fb-82bd-bbfb55db6916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.98\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.99      0.99      0.99       672\n",
      "     Healthy       0.93      0.94      0.93        97\n",
      "\n",
      "    accuracy                           0.98       769\n",
      "   macro avg       0.96      0.96      0.96       769\n",
      "weighted avg       0.98      0.98      0.98       769\n",
      "\n",
      "\n",
      "Confusion Matrix (SVM):\n",
      "[[665   7]\n",
      " [  6  91]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X and y are already prepared as in the previous code\n",
    "\n",
    "# It's a good practice to scale the data for SVM\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm_clf = SVC(kernel='linear', random_state=42)  # You can also try other kernels like 'rbf' or 'poly'\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (SVM):\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6108d888-5683-4776-a1cb-0ede62babd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 1.00\n",
      "\n",
      "Classification Report (k-NN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       1.00      1.00      1.00       672\n",
      "     Healthy       1.00      0.99      0.99        97\n",
      "\n",
      "    accuracy                           1.00       769\n",
      "   macro avg       1.00      0.99      1.00       769\n",
      "weighted avg       1.00      1.00      1.00       769\n",
      "\n",
      "\n",
      "Confusion Matrix (k-NN):\n",
      "[[672   0]\n",
      " [  1  96]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# It's good practice to scale the data for k-NN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the k-NN classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)  # You can change the number of neighbors (k)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"k-NN Accuracy: {accuracy_knn:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (k-NN):\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (k-NN):\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef26030-ee5d-42ee-b32e-9e4b7eba57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean       std     min        max  skewness  kurtosis   entropy  \\\n",
      "0  0.226521  5.872357 -19.021  16.051001  0.152169 -0.058698  3.218837   \n",
      "1 -0.024865  6.907847 -19.937  29.587000  0.099320  0.295053  3.301411   \n",
      "2  0.154342  7.221072 -29.268  24.389999 -0.024903  0.382880  3.277793   \n",
      "3  0.479828  9.444859 -25.000  30.768999  0.596013  0.354002  3.461565   \n",
      "4  0.481580  9.809538 -21.569  34.313999  0.625237  0.290154  3.487736   \n",
      "\n",
      "  condition        state    patient                                  electrode  \n",
      "0        AD  Eyes_closed  Paciente1          C3 - Central Left (Motor Control)  \n",
      "1        AD  Eyes_closed  Paciente1         C4 - Central Right (Motor Control)  \n",
      "2        AD  Eyes_closed  Paciente1  Cz - Central Midline (Motor Coordination)  \n",
      "3        AD  Eyes_closed  Paciente1        F1 - Frontal Left (Decision Making)  \n",
      "4        AD  Eyes_closed  Paciente1       F2 - Frontal Right (Decision Making)  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "\n",
    "# Function to extract features from a single signal\n",
    "def extract_features(signal):\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(signal)\n",
    "    features['std'] = np.std(signal)\n",
    "    features['min'] = np.min(signal)\n",
    "    features['max'] = np.max(signal)\n",
    "    features['skewness'] = skew(signal)\n",
    "    features['kurtosis'] = kurtosis(signal)\n",
    "    # Calculate entropy using the probability density of the signal\n",
    "    # Normalize the signal values to sum to 1\n",
    "    signal_prob = np.histogram(signal, bins=50, density=True)[0]\n",
    "    features['entropy'] = entropy(signal_prob + 1e-8)  # Adding small value to avoid log(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply the feature extraction to all EEG signals\n",
    "extracted_features = []\n",
    "for signal in all_data:\n",
    "    features = extract_features(signal)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the extracted features into a DataFrame\n",
    "df_features = pd.DataFrame(extracted_features)\n",
    "\n",
    "# Add the labels (condition, state, patient, electrode)\n",
    "df_features['condition'] = [label[0] for label in labels]\n",
    "df_features['state'] = [label[1] for label in labels]\n",
    "df_features['patient'] = [label[2] for label in labels]\n",
    "df_features['electrode'] = [label[3] for label in labels]\n",
    "\n",
    "# Display the first few rows of the features DataFrame\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec1050a-6c87-4168-8045-39c62d07ebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.95\n",
      "\n",
      "Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      0.98      0.97       672\n",
      "     Healthy       0.84      0.72      0.78        97\n",
      "\n",
      "    accuracy                           0.95       769\n",
      "   macro avg       0.90      0.85      0.87       769\n",
      "weighted avg       0.95      0.95      0.95       769\n",
      "\n",
      "\n",
      "Confusion Matrix (Decision Tree):\n",
      "[[659  13]\n",
      " [ 27  70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming df_features contains the extracted features and labels\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_features[['mean', 'std', 'min', 'max', 'skewness', 'kurtosis', 'entropy']]  # Features\n",
    "y = df_features['condition']  # Labels (AD or Healthy)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (Decision Tree):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Decision Tree):\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a418386d-d1e8-4cc5-824c-caa635e9d74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.97\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.97      1.00      0.98       672\n",
      "     Healthy       0.96      0.79      0.87        97\n",
      "\n",
      "    accuracy                           0.97       769\n",
      "   macro avg       0.97      0.89      0.93       769\n",
      "weighted avg       0.97      0.97      0.97       769\n",
      "\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "[[669   3]\n",
      " [ 20  77]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming df_features contains the extracted features and labels\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_features[['mean', 'std', 'min', 'max', 'skewness', 'kurtosis', 'entropy']]  # Features\n",
    "y = df_features['condition']  # Labels (AD or Healthy)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef5480c-9434-449f-9e81-5a4a6e7db85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.93\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.93      1.00      0.96       672\n",
      "     Healthy       0.98      0.49      0.66        97\n",
      "\n",
      "    accuracy                           0.93       769\n",
      "   macro avg       0.96      0.75      0.81       769\n",
      "weighted avg       0.94      0.93      0.93       769\n",
      "\n",
      "\n",
      "Confusion Matrix (SVM):\n",
      "[[671   1]\n",
      " [ 49  48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming df_features contains the extracted features and labels\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_features[['mean', 'std', 'min', 'max', 'skewness', 'kurtosis', 'entropy']]  # Features\n",
    "y = df_features['condition']  # Labels (AD or Healthy)\n",
    "\n",
    "# Scale the features since SVM is sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm_clf = SVC(kernel='linear', random_state=42)  # Using a linear kernel for simplicity\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (SVM):\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a7dee2-a7ee-4ef9-a0d1-6d29b272f466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.96\n",
      "\n",
      "Classification Report (k-NN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.97      0.99      0.97       672\n",
      "     Healthy       0.88      0.75      0.81        97\n",
      "\n",
      "    accuracy                           0.96       769\n",
      "   macro avg       0.92      0.87      0.89       769\n",
      "weighted avg       0.95      0.96      0.95       769\n",
      "\n",
      "\n",
      "Confusion Matrix (k-NN):\n",
      "[[662  10]\n",
      " [ 24  73]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming df_features contains the extracted features and labels\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_features[['mean', 'std', 'min', 'max', 'skewness', 'kurtosis', 'entropy']]  # Features\n",
    "y = df_features['condition']  # Labels (AD or Healthy)\n",
    "\n",
    "# Scale the features since k-NN relies on distance metrics\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the k-NN classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)  # Using k=5, but you can experiment with different k values\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"k-NN Accuracy: {accuracy_knn:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (k-NN):\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (k-NN):\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18821a93-fc5d-4a17-8668-8eee0045a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 334.4225 - val_loss: 387.7471\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 336.6559 - val_loss: 387.6814\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 328.2199 - val_loss: 387.6299\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 337.7356 - val_loss: 387.6255\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 329.0984 - val_loss: 387.6143\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 331.0448 - val_loss: 387.6109\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 333.3795 - val_loss: 387.6089\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 312.9789 - val_loss: 387.6057\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.2866 - val_loss: 387.6048\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 326.1411 - val_loss: 387.6062\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 334.8688 - val_loss: 387.6046\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 329.1277 - val_loss: 387.6001\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 324.0519 - val_loss: 387.5966\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 332.8795 - val_loss: 387.5963\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 324.4841 - val_loss: 387.5947\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 319.8904 - val_loss: 387.5945\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 324.3206 - val_loss: 387.5939\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329.3561 - val_loss: 387.5946\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 320.3398 - val_loss: 387.5929\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 320.4898 - val_loss: 387.5951\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 315.9518 - val_loss: 387.5943\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 339.3293 - val_loss: 387.5939\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 319.4532 - val_loss: 387.5862\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 337.4024 - val_loss: 387.5808\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 326.7910 - val_loss: 387.5845\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 321.0095 - val_loss: 387.5804\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 327.7578 - val_loss: 387.5772\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 336.4226 - val_loss: 387.5764\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 325.3572 - val_loss: 387.5791\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 334.0162 - val_loss: 387.5735\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 338.6224 - val_loss: 387.5700\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 330.5688 - val_loss: 387.5697\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326.6984 - val_loss: 387.5629\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 350.1193 - val_loss: 387.5593\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 333.1276 - val_loss: 387.5601\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 325.8840 - val_loss: 387.5532\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 327.5423 - val_loss: 387.5517\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 334.1304 - val_loss: 387.5478\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326.2016 - val_loss: 387.5450\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 325.0759 - val_loss: 387.5440\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326.8943 - val_loss: 387.5372\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 321.3260 - val_loss: 387.5325\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 326.1855 - val_loss: 387.5314\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 330.3156 - val_loss: 387.5291\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 327.6848 - val_loss: 387.5250\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326.8914 - val_loss: 387.5236\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 318.4142 - val_loss: 387.5221\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 329.8597 - val_loss: 387.5127\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 327.7352 - val_loss: 387.5094\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 318.8039 - val_loss: 387.5070\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step\n",
      "Shape of compressed data: (3841, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your original EEG data (e.g., 1024 values per signal)\n",
    "input_dim = X.shape[1]  # Number of input features (e.g., 1024)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)  # Compress to 128 dimensions\n",
    "encoded = Dense(64, activation='relu')(encoded)       # Further compress to 64 dimensions (bottleneck)\n",
    "\n",
    "decoded = Dense(128, activation='relu')(encoded)      # Expand back to 128 dimensions\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)  # Output layer (reconstructed input)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Define the encoder model to extract the compressed features\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# Extract the compressed features (latent representation)\n",
    "X_compressed = encoder.predict(X)\n",
    "\n",
    "# Print the shape of the compressed data\n",
    "print(f\"Shape of compressed data: {X_compressed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bacba3d-22db-4c3e-a734-a98cca9ff5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\asus\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90026b59-091f-4c97-87f7-5e861f3af366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on Compressed Data: 0.94\n",
      "\n",
      "Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      0.97      0.97       672\n",
      "     Healthy       0.77      0.74      0.76        97\n",
      "\n",
      "    accuracy                           0.94       769\n",
      "   macro avg       0.87      0.86      0.86       769\n",
      "weighted avg       0.94      0.94      0.94       769\n",
      "\n",
      "\n",
      "Confusion Matrix (Decision Tree):\n",
      "[[651  21]\n",
      " [ 25  72]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming y contains the labels (AD or Healthy)\n",
    "\n",
    "# Split the compressed data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_compressed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy on Compressed Data: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (Decision Tree):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Decision Tree):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a63f073-af27-46a0-ba77-423be8d1fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy on Compressed Data: 0.96\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      1.00      0.98       672\n",
      "     Healthy       0.96      0.68      0.80        97\n",
      "\n",
      "    accuracy                           0.96       769\n",
      "   macro avg       0.96      0.84      0.89       769\n",
      "weighted avg       0.96      0.96      0.95       769\n",
      "\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "[[669   3]\n",
      " [ 31  66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_compressed is the compressed data from the autoencoder\n",
    "# and y contains the labels (AD or Healthy)\n",
    "\n",
    "# Split the compressed data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_compressed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy on Compressed Data: {accuracy_rf:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52f5615b-8197-452b-90e5-d2be0dd827a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy on Compressed Data: 0.94\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.93      1.00      0.96       672\n",
      "     Healthy       0.96      0.52      0.67        97\n",
      "\n",
      "    accuracy                           0.94       769\n",
      "   macro avg       0.95      0.76      0.82       769\n",
      "weighted avg       0.94      0.94      0.93       769\n",
      "\n",
      "\n",
      "Confusion Matrix (SVM):\n",
      "[[670   2]\n",
      " [ 47  50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_compressed is the compressed data from the autoencoder\n",
    "# and y contains the labels (AD or Healthy)\n",
    "\n",
    "# Scale the compressed features as SVM is sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_compressed)\n",
    "\n",
    "# Split the scaled data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm_clf = SVC(kernel='linear', random_state=42)  # Using a linear kernel for simplicity\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy on Compressed Data: {accuracy_svm:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (SVM):\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b168d2b9-0a37-4de6-a63f-cf93ebc5e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy on Compressed Data: 0.95\n",
      "\n",
      "Classification Report (k-NN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.96      0.99      0.97       672\n",
      "     Healthy       0.88      0.70      0.78        97\n",
      "\n",
      "    accuracy                           0.95       769\n",
      "   macro avg       0.92      0.84      0.88       769\n",
      "weighted avg       0.95      0.95      0.95       769\n",
      "\n",
      "\n",
      "Confusion Matrix (k-NN):\n",
      "[[663   9]\n",
      " [ 29  68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_compressed is the compressed data from the autoencoder\n",
    "# and y contains the labels (AD or Healthy)\n",
    "\n",
    "# Scale the compressed features since k-NN relies on distance metrics\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_compressed)\n",
    "\n",
    "# Split the scaled data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the k-NN classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)  # Using k=5, but you can experiment with different values of k\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"k-NN Accuracy on Compressed Data: {accuracy_knn:.2f}\")\n",
    "\n",
    "# Display classification report for precision, recall, F1-score\n",
    "print(\"\\nClassification Report (k-NN):\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix (k-NN):\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b98c7-0a4f-4874-8798-9c2364d95506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
